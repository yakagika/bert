{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dbe4d4d",
   "metadata": {},
   "source": [
    "2-1 トークン化と前処理\n",
    "\n",
    "- トークン化  \n",
    "    文を適当な単位に分割すること.\n",
    "    \n",
    "    \n",
    "- トークナイザ  \n",
    "    トークン化のためのツール\n",
    "    \n",
    "    \n",
    "- トークン  \n",
    "    分割によって得られた文の構成要素\n",
    "    \n",
    "    \n",
    "- 語彙  \n",
    "    トークンとIDのマップ \n",
    "    コレに含まれない語彙は未知語\n",
    "\n",
    "\n",
    "- コーパス  \n",
    "    大量の文字を収集したデータベース  \n",
    "    注釈付きコーパス  \n",
    "        品詞タグなどの情報を付加したコーパス  \n",
    "\n",
    "\n",
    "- 代表的なトークン化の技法  \n",
    "\n",
    "    - 単語分割  \n",
    "        文章を単語に分割する  \n",
    "        人間の直感に近い  \n",
    "            ただし, 語彙数を増やすことでしか未知語に対応できない\n",
    "        \n",
    "        - 日本語の単語分割システム = 形態素解析ツール\n",
    "            Ex. MeCab, Sudachi, Juman  \n",
    "           形態素(単語)に分割し, 品詞や活用における基本形などの統語的情報を付与  \n",
    "           \n",
    "            \n",
    "    - 文字分割  \n",
    "        文章を1文字毎に分割  \n",
    "        語彙が小さくなるが意味が付与されないのでNNで考慮する必要あり\n",
    "    \n",
    "    - サブワード分割  \n",
    "        単語をさらに部分文字列(サブワード)に分割  \n",
    "        BERTではこれを利用している \n",
    "        適切に語彙を選べば単語分割に比べて少ない語彙数で多くの入力を表現可能 \n",
    "        未知語に強い  \n",
    "        Ex.  \n",
    "        [\"東京タワー\",\"京都タワー\",\"大阪大学\"]  \n",
    "        ->  \n",
    "        [\"東京\",\"京都\",\"大阪\",\"##タワー\",\"##大学\"]  \n",
    "        \"##\" は単語の途中に現れる要素  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9c8cd",
   "metadata": {},
   "source": [
    "2-2 ニューラル言語モデル  \n",
    "\n",
    "\n",
    "- N言語モデル  \n",
    "    NNによって実現される言語モデル  \n",
    "    汎用的な言語的特徴(トークンの意味, 品詞などの倒語情報)を表す  \n",
    "  \n",
    "\n",
    "- 言語モデル    \n",
    "    文章の出現しやすさ(自然さ)を確率によってモデル化したもの  \n",
    "    $p(\"私はパンを食べた\") > p(\"私は家を食べた\")$  \n",
    "    左の文章の方が自然  \n",
    "    \n",
    "     文章S  \n",
    "     文章Sのトークン $(w_1,w_2,...,w_3)$  \n",
    "     のとき  \n",
    "     \n",
    "$$  \n",
    "    \\begin{align}\n",
    "         p(S) &= p(w_1,w_2,...,w_3) \\\\\n",
    "              &= p(w_1) \\times p(w_2|w_1) \\times p(w_3|w_1,w_2)...\\\\\n",
    "              &= \\Pi_{i=1}^{n}p(w_i|c_i)\\\\\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "あるトークンの出現率が,それ以前に出現したトークンに依存している.  \n",
    "\n",
    "- 文脈(context)  \n",
    "    $c_i = (w_1,w_2,...,w_i-1)$ は $w_i$ を予測するための前提条件  \n",
    "    \n",
    "文章の出現確率は, $p(w_i|c_i)$として表される.  \n",
    "\n",
    "\n",
    "- 条件付き確率  \n",
    "    ある事象が起こるという条件のもとで,別のある事象が起こる確率  \n",
    "    $ P(A|B) = \\frac{p(A \\cap B)}{P(B)}$  \n",
    "    \n",
    "    \n",
    "- ニューラルネットワーク  \n",
    "    神経回路網をモデル化する試みに起源を持つ数理モデル  \n",
    "    なんらかの変換を行う層(layer)の組み合わせによって構成されている  \n",
    "    \n",
    "    \n",
    "        - 層  \n",
    "            入力値に対して適当な線形変換,活性化関数による非線形変換の合成変換が施された出力値を返す    \n",
    "            \n",
    "            \n",
    "        - パラメータ  \n",
    "            層が持つ調整可能な変数, 変換を決定する  \n",
    "            ｢特定のパラメタを持つNNの入出力関係｣と｢理想的な入出力関係｣の間のズレを損失関数で計測して,最小化することで調整   \n",
    "            \n",
    "        - ハイパーパラメータ  \n",
    "            層の組み合わせ方,次元数,などの設定事項  \n",
    "            \n",
    "\n",
    "- 多クラス分類  \n",
    "\n",
    "    $N$ : カテゴリー数  \n",
    "    $x$ : データを表現するベクトル  \n",
    "    $ l \\in {1,2,..,N}$ :  カテゴリーのラベル  \n",
    "    $y = F(x,\\theta)$ xを入力した差異のNNの出力  \n",
    "    $ \\theta $ : パラメタ  \n",
    "    $ y = [y_1,y_2,...y_N] $ : N次元ベクトル,各$y_i$はカテゴリーへの予測の確度(分類スコア)  \n",
    "    $ y_i s.t \\forall y_i > y_j, y_i,y_j \\in y$: NNの予測  \n",
    "        \n",
    "        \n",
    "<img src=\"graph/ch2/ss2-2.png\">   \n",
    "\n",
    "\n",
    "モデルの学習  \n",
    "    損失関数(yとlのズレ)$L(y_i,l_i)$を最小化する.  \n",
    "    SoftMax関数により, 分類スコア → 予測確率に変換  \n",
    "    \n",
    "\n",
    "$$\n",
    "p_i = \\frac{exp(y_i)}{\\sum_{j=1}^{N}exp(y_i)} (i=1,2,...,N))\\\\\n",
    "0 <= p_i <= 1 \\\\\n",
    "\\sum_{i}^{N} = 1 \\\\\n",
    "$$\n",
    "\n",
    "$(p_1,...,p_N)$が確率分布とみなせる\n",
    "\n",
    "\n",
    "<img src=\"graph/ch2/ssSoftMax1.png\">\n",
    "<img src=\"graph/ch2/ssSoftMax2.png\">\n",
    "<img src=\"graph/ch2/ssSoftMax3.png\">\n",
    "\n",
    "\n",
    "ref. http://deepokayama.kenkyuukai.jp/FilePreview_Subject.asp?id=6414&sid=2063&cid=352&ref=%2Fsubject%2Fsubject%5Flist%2Easp \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "損失  \n",
    "    クロスエントロピー損失(交差エントロピー損失)  \n",
    "    \n",
    "$$\n",
    "L(y,l) = -log p_i \\\\\n",
    "L=\\frac{1}{m}\\sum_{i=1}^{m}L(y_i,l_i)\n",
    "$$\n",
    "m: サンプルサイズ\n",
    "\n",
    "$0 < p_i < 1$ なので対数をとると, 確率が悪い場合に損失が非常に大きくなる.\n",
    "\n",
    "<img src=\"graph/ch2/ssCrossE.png\">\n",
    "ref. https://manareki.com/crossentropy_lossfunction\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47b672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bcd3374",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
