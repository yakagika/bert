{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a6e8e9",
   "metadata": {},
   "source": [
    "4-1 計算環境:  Google Colaboratory\n",
    "\n",
    "メリット\n",
    "    - 環境構築に手間がかからない  \n",
    "    - ライブラリが入っている\n",
    "    - GPU計算が簡単\n",
    "    \n",
    "\n",
    "利用するライブラリ\n",
    "\n",
    "- transformaers  \n",
    "    ニューラル言語モデルのライブラリ  \n",
    "- Fugashi  \n",
    "    MeCabのPython用ライブラリ  \n",
    "- ipadic  \n",
    "    MeCabで形態素解析を行う際に用いる辞書  \n",
    "    \n",
    "Colaboratory では, !の後にコマンドを書くとコマンド実行できる\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8e0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import BertJapaneseTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931120a4",
   "metadata": {},
   "source": [
    "- Transformers  \n",
    "    - Huggingface 社が提供しているN言語モデルのOSS  \n",
    "    - cl-touhoku/bert-base-japanese-whole-word-masking  \n",
    "        - 東北大学がWikipediaの日本語記事データを用いて学習したモデル  \n",
    "        \n",
    "    - 処理手順  \n",
    "    1. トークナイザを用いて,文章をトークン化\n",
    "    2. 処理したデータをBERTに入力し,出力を得る  \n",
    "    \n",
    "- トークナイザ  \n",
    "    文章をトークンに分割し,BERTに入力できる形に変換."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e804e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e4764f83c346eda13a6e77d718e966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/252k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cee048bb77744deb9fb7c07dd1e59f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/110 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb3e6abd7a14247ae5f7b6889fb0bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/479 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "# 学習済みのトークナイザをロード\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd51356",
   "metadata": {},
   "source": [
    "内部では, \n",
    "1. MeCabで単語分割  \n",
    "2. WOrdPieceでトークン化  \n",
    "している."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51226ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['明日', 'は', '自然', '言語', '処理', 'の', '勉強', 'を', 'しよ', 'う', '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#トークン化してみる\n",
    "tokenizer.tokenize('明日は自然言語処理の勉強をしよう.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e029afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['明日', 'は', 'マシン', '##ラー', '##ニング', 'の', '勉強', 'を', 'しよ', 'う', '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('明日はマシンラーニングの勉強をしよう.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40650fdf",
   "metadata": {},
   "source": [
    "｢##｣はサブワード分割の結果,最初の単語以外につく."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a6ed06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['機械', '学習', 'を', '中国', '語', 'に', 'する', 'と', '机', '器', '学', '[UNK]', 'だ', '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('機械学習を中国語にすると机器学习だ.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb2ce7",
   "metadata": {},
   "source": [
    "习が未知語[UNK]トークンに変換されている."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf17afa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 11475, 9, 1757, 1882, 2762, 5, 8192, 11, 2132, 205, 143, 3]\n"
     ]
    }
   ],
   "source": [
    "#文章を符号化(ID化)するには encode()を使う.\n",
    "input_ids = tokenizer.encode('明日は自然言語処理の勉強をしよう.')\n",
    "print(input_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9611097",
   "metadata": {},
   "source": [
    "encode()は,  \n",
    "- トークン列の先頭に[CLS]  \n",
    "- 末尾に[SEP]  \n",
    "を自動で追加する.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "110922f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '明日', 'は', '自然', '言語', '処理', 'の', '勉強', 'を', 'しよ', 'う', '.', '[SEP]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ID ー> トークン は convert_ids_to_tokens()\n",
    "tokenizer.convert_ids_to_tokens(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb8cc75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding:\n",
      "{'input_ids': [2, 11475, 5, 11385, 9, 16577, 75, 143, 3, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}\n",
      "# tokens:\n",
      "['[CLS]', '明日', 'の', '天気', 'は', '晴れ', 'だ', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "#BERTを利用するためにトークン列の長さ(系列長)を同じに揃える\n",
    "#系列長が短ければ特殊トークン[PAD]を末尾に足す\n",
    "#ながければ末尾トークンを削る\n",
    "#[PAD]部分は処理に関係ないので,\n",
    "#関係ある部分のみを表すattenntion_maskを用意しておく\n",
    "\n",
    "# tokenizerで指定した系列帳に揃えられたidやmaskの辞書が手に入る\n",
    "# max_length : 系列長\n",
    "# padding='max_length': 足りなかったら12まで[PAD]を足す\n",
    "# truncation: 多かったら削る\n",
    "\n",
    "text = '明日の天気は晴れだ.'\n",
    "encoding = tokenizer(\n",
    "    text, max_length=12, padding='max_length', truncation=True\n",
    ")\n",
    "print('# encoding:')\n",
    "print(encoding)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "print('# tokens:')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb4e2d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding:\n",
      "{'input_ids': [2, 11475, 5, 11385, 9, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "# tokens:\n",
      "['[CLS]', '明日', 'の', '天気', 'は', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#系列長を短くしてみる\n",
    "encoding = tokenizer(\n",
    "    text, max_length=6, padding='max_length', truncation=True\n",
    ")\n",
    "print('# encoding:')\n",
    "print(encoding)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "print('# tokens:')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db6b2750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding:\n",
      "{'input_ids': [2, 11475, 5, 11385, 9, 16577, 75, 143, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "# tokens:\n",
      "['[CLS]', '明日', 'の', '天気', 'は', '晴れ', 'だ', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#truncationをFalseにしてみる\n",
    "encoding = tokenizer(\n",
    "    text, max_length=6, padding='max_length', truncation=False\n",
    ")\n",
    "print('# encoding:')\n",
    "print(encoding)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "print('# tokens:')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbc5e6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# encoding:\n",
      "{'input_ids': tensor([[    2, 11475,     5, 11385,     9, 16577,    75,   143,     3,     0],\n",
      "        [    2,  6311,    14,  1132,     7, 16084,   332,    58,    10,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "#復数の文章をリストで渡せる\n",
    "text_list = [ '明日の天気は晴れだ.'\n",
    "             ,'パソコンが急に動かなくなった.' ]\n",
    "\n",
    "#それぞれリストが返ってくる\n",
    "# padding=longest  で最も長い文章が系列長になる\n",
    "\n",
    "# BERTに入力する場合は数値配列はPyTorchの多次元配列を扱うための型\n",
    "# torch.Tensorに変換する必要がある\n",
    "# return_tensors='pt'でテンソルとして出力されそのまま入力可能\n",
    "encoding = tokenizer( text_list\n",
    "                     , max_length = 10\n",
    "                     , padding='max_length'\n",
    "                     , truncation=True\n",
    "                     , return_tensors='pt'\n",
    ")\n",
    "print('# encoding:')\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e34857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected one of cpu, cuda, xpu, mkldnn, opengl, opencl, ideep, hip, msnpu, xla, vulkan device type at start of device string: mps",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v3/8sj56t4917zck76jw740zg780000gn/T/ipykernel_56743/897326183.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#BERTをGPUに載せる\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, xpu, mkldnn, opengl, opencl, ideep, hip, msnpu, xla, vulkan device type at start of device string: mps"
     ]
    }
   ],
   "source": [
    "#符号化(トークン化→ID化)されたデータをBERTに入力する\n",
    "#モデルのロード\n",
    "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "torch.device('mps')\n",
    "\n",
    "#BERTをGPUに載せる\n",
    "bert = bert.cuda()\n",
    "\n",
    "#モデルの概要\n",
    "print(bert.congif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e9cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
